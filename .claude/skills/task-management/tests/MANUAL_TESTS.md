# Manual Tests - Session 1

Tests manuels pour valider le fonctionnement du skill task-management.

## Session 1: task-next + Infrastructure

### Test 1: Suggestion de t√¢che basique

**Setup:** Avoir plusieurs t√¢ches "‚è≥ √Ä faire" dans `.tasks/tasks/`

**Commande:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/algorithms/priority_scorer.py
```

**Attendu:**

- Affiche "üí° Prochaine t√¢che sugg√©r√©e: [ID]"
- Affiche le top 3 des t√¢ches
- Scores d√©croissants
- Formule WSJF visible (valeur/temps)

**Status:** [x]

---

### Test 2: Aucune t√¢che disponible

**Setup:** Mettre toutes les t√¢ches en "üîÑ En cours" ou "‚úÖ Termin√©" temporairement

**Commande:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/algorithms/priority_scorer.py
```

**Attendu:**

- Message: "‚ö†Ô∏è  Aucune t√¢che en attente!"
- Pas d'erreur

**Status:** [ ]

*Note utilisateur:*

```markdown
Il est difficile de mettre toutes les t√¢ches √† "En cours" √©tant donn√© que ce sont de vraies t√¢ches. Ce serait bien de revoir ce test en cr√©ant un environnement isol√© (jouer sur les chemins dans `.claude/skills/task-management/config/paths.yml`)
```

---

### Test 3: Configuration YAML loading

**Commande:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/config_loader.py
```

**Attendu:**

- Charge priorities.yml (üî¥ Haute = score 10)
- Charge trigrammes.yml (7 trigrammes)
- Charge paths.yml
- "‚úÖ All configs loaded successfully!"

**Status:** [x]

---

### Test 4: File parser sur t√¢che r√©elle

**Commande:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/file_parser.py
```

**Attendu:**

- Parse une t√¢che de `.tasks/tasks/`
- Extrait m√©tadonn√©es (ID, Statut, Priorit√©)
- Compte sous-t√¢ches (X/Y completed)
- Extrait sections (Description, Notes)

**Status:** [x]

---

### Test 5: Tests unitaires

**Commande:**

```bash
cd .claude/skills/task-management && uv run --with pytest python3 -m pytest tests/test_file_parser.py -v
```

**Attendu:**

- 7 tests passent
- test_parse_metadata_table PASSED
- test_extract_subtasks PASSED
- test_parse_task_file PASSED
- Etc.

**Status:** [x]

*Note utilisateur:*

```markdown
1. il serait mieux d'utiliser la commande `uvx pytest tests/test_file_parser.py -v`, plus courte
2. Note pour plus tard: Je suis partag√© par le fait d'ex√©cuter les tests depuis `.claude/skills/task-management`. En effet, √† la fois cela permet de garder une isolation de ces skills dans le but de les r√©utiliser, et en m√™me temps, au sein de ce projet, utiliser une "2e racine" me semble compliqu√©. (ce probl√®me est peut-√™tre li√© √† l'√©chec du test 4)
```

---

### Test 6: Output JSON

**Commande:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/algorithms/priority_scorer.py --json --top 3
```

**Attendu:**

- Output JSON valide
- Contient id, title, priority, score, breakdown
- Top 3 t√¢ches

**Status:** [x]

---

## Notes

- Tous les tests doivent √™tre ex√©cut√©s depuis la racine du projet (`/Users/bastiengallay/Dev/cv/neat-cv/`)
- Les chemins relatifs dans les configs supposent cette racine
- `uv run --with pyyaml` installe automatiquement les d√©pendances

### Tests isol√©s (sans affecter la production)

Pour tester sans modifier les vraies t√¢ches, utilisez des variables d'environnement :

```bash
# Cr√©er un r√©pertoire de test
mkdir -p .tasks/test-tasks

# Copier une t√¢che d'exemple
cp .tasks/tasks/CNT-001*.md .tasks/test-tasks/

# Ex√©cuter avec override
TASK_SYSTEM_TASKS_DIR=.tasks/test-tasks uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/file_parser.py
```

Variables d'environnement disponibles :

- `TASK_SYSTEM_TASKS_DIR` - R√©pertoire des t√¢ches
- `TASK_SYSTEM_ARCHIVED_DIR` - R√©pertoire d'archives
- `TASK_SYSTEM_DASHBOARD` - Chemin vers TASKS.md

## Validation Finale

- [ ] Tous les tests manuels passent
- [ ] Aucune erreur Python
- [ ] Output conforme aux attentes
- [x] Configuration charg√©e correctement
- [x] Algorithme WSJF produit des r√©sultats coh√©rents

**Valid√© par:** _______
**Date:** _______

---

## Session 2: Core Infrastructure + Validators

### Test 7: ID Generator

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/id_generator.py
```

**Attendu:**

- Shows existing IDs by trigramme
- Generates next IDs for each trigramme
- Tests slug generation
- All operations succeed

**Status:** [x]

---

### Test 8: DoR Validator (Valid Task)

**Setup:** Have a valid task in "‚è≥ √Ä faire" status

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/validators/dor_validator.py CNT-005
```

**Attendu:**

- ‚úÖ Valid: Yes
- No error-level issues
- May have warnings (acceptable)

**Status:** [x]

*Note utilisateur:*

```markdown
Il faudrait pouvoir faire ce teste de mani√®re isol√© de l'environnement de prod.
```

---

### Test 9: DoR Validator (Invalid Task)

**Setup:** Try with a task that's "üîÑ En cours"

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/validators/dor_validator.py <TASK-IN-PROGRESS>
```

**Attendu:**

- ‚ùå Valid: No
- Error: "Task already in progress"

**Status:** [x]

*Note utilisateur:*

```markdown
Il faudrait pouvoir faire ce teste de mani√®re isol√© de l'environnement de prod.
```

---

### Test 10: DoD Validator (In Progress Task)

**Setup:** Have a task in "üîÑ En cours" with all subtasks checked

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/validators/dod_validator.py INF-004
```

**Attendu:**

- Shows validation results
- Checks status, subtasks, result section
- Exit code 0 if valid, 1 if invalid

**Status:** [x]

*Note utilisateur:*

```markdown
Il faudrait pouvoir faire ce teste de mani√®re isol√© de l'environnement de prod.
```

---

### Test 11: Dashboard Updater

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/dashboard_updater.py
```

**Attendu:**

- Reads TASKS.md successfully
- Shows count of active/completed tasks
- No errors

**Status:** [x]

---

### Test 12: Git Operations

**Command:**

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/core/git_operations.py
```

**Attendu:**

- Shows current branch
- Shows if uncommitted changes
- Formats commit message correctly

**Status:** [x]

---

### Test 13: Unit Tests (ID Generator)

**Command:**

```bash
cd .claude/skills/task-management && uvx --with pyyaml pytest tests/test_id_generator.py -v
```

**Attendu:**

- 25 tests pass
- No failures

**Status:** [x]

---

### Test 14: Unit Tests (DoR Validator)

**Command:**

```bash
cd .claude/skills/task-management && uvx --with pyyaml pytest tests/test_dor_validator.py -v
```

**Attendu:**

- 23 tests pass
- No failures

**Status:** [x]

---

## Validation Finale Session 2

- [ ] All Session 1 tests still pass
- [ ] All Session 2 manual tests pass
- [ ] ID generator works correctly
- [ ] DoR validator catches invalid states
- [ ] DoD validator validates completion criteria
- [ ] Dashboard operations work
- [ ] Git operations work
- [ ] All unit tests pass

**Valid√© par:** _______
**Date:** _______

---

## Session 3: Analysis & Lifecycle Workflows

### Test 15: Recommendation Parser (Unit Tests)

**Command:**

```bash
cd .claude/skills/task-management && uvx --with pyyaml pytest tests/test_recommendation_parser.py -v
```

**Attendu:**

- 19 tests pass
- No failures

**Status:** [x]

---

### Test 16: All Tests Suite

**Command:**

```bash
cd .claude/skills/task-management && uvx --with pyyaml pytest tests/ -v
```

**Attendu:**

- 74 tests pass (25 id_generator + 23 dor_validator + 7 file_parser + 19 recommendation_parser)
- No failures

**Status:** [x]

---

## Validation Finale Session 3

- [x] All Session 1+2 tests still pass
- [x] Recommendation parser tests pass (19/19)
- [x] Complete test suite passes (74/74)
- [ ] Workflows documented and accessible
- [ ] No regression in existing functionality

**Valid√© par:** _______
**Date:** _______

---

## Session 4: Tests finaux + Documentation + Cleanup

### Test 17: Suite compl√®te de tests unitaires

**Command:**

```bash
cd .claude/skills/task-management && uvx --with pyyaml pytest tests/ -v --tb=short
```

**Attendu:**

- 74 tests passent (file_parser, priority_scorer, id_generator, dor_validator, recommendation_parser)
- Aucune failure
- Coverage complet des fonctionnalit√©s critiques

**Status:** [x]

*Note utilisateur:*

```markdown
[√Ä compl√©ter lors du test]
```

---

### Test 18: V√©rification suppression slash commands

**Command:**

```bash
ls -la .claude/commands/task-*.md 2>/dev/null
```

**Attendu:**

- Aucun fichier trouv√©
- Message "no matches found" ou aucun output
- 9 fichiers supprim√©s (~2,340 lignes)

**Status:** [x]

*Note utilisateur:*

```markdown
[√Ä compl√©ter lors du test]
```

---

### Test 19: Documentation CLAUDE.md mise √† jour

**Command:**

```bash
grep -n "task-management" CLAUDE.md
```

**Attendu:**

- Trouve la documentation du skill
- Architecture document√©e
- Workflows list√©s

**Status:** [x]

*Note utilisateur:*

```markdown
[√Ä compl√©ter lors du test]
```

---

### Test 20: Workflow end-to-end complet

**Pr√©paration:** Des fichiers de test r√©utilisables sont disponibles dans `tests/fixtures/`

**Setup:** Cr√©er un environnement isol√© de test

```bash
# 1. Cr√©er l'environnement de test
export TASK_SYSTEM_TASKS_DIR=.tasks/test-e2e
export TASK_SYSTEM_DASHBOARD=.tasks/test-e2e/TASKS.md
export TASK_SYSTEM_ARCHIVED_DIR=.tasks/test-e2e/.archived

mkdir -p .tasks/test-e2e
mkdir -p .tasks/test-e2e/.archived

# 2. Copier les fichiers de test pr√©par√©s
cp .claude/skills/task-management/tests/fixtures/TEST-001-sample-task.md .tasks/test-e2e/
cp .claude/skills/task-management/tests/fixtures/TASKS-test-dashboard.md .tasks/test-e2e/TASKS.md

# 3. V√©rifier que l'environnement est pr√™t
ls -la .tasks/test-e2e/
```

**Tests pas √† pas:**

#### √âtape 1: V√©rifier que task-next trouve la t√¢che TEST-001

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/algorithms/priority_scorer.py
```

**‚úÖ Validation:** Doit afficher "üí° Prochaine t√¢che sugg√©r√©e: TEST-001"

---

#### √âtape 2: Valider la t√¢che avec DoR validator

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/validators/dor_validator.py TEST-001
```

**‚úÖ Validation:** Doit afficher "‚úÖ Valid: Yes" (la t√¢che est en statut "‚è≥ √Ä faire")

---

#### √âtape 3: Simuler le d√©marrage de la t√¢che

```bash
# Mettre √† jour le statut dans le fichier de t√¢che
sed -i '' 's/‚è≥ √Ä faire/üîÑ En cours/' .tasks/test-e2e/TEST-001-sample-task.md

# Mettre √† jour le dashboard (d√©placer de "√Ä faire" vers "En cours")
uv run --with pyyaml python3 -c "
import sys
sys.path.append('.claude/skills/task-management/scripts')
from core.dashboard_updater import update_task_status

# La fonction attend que la t√¢che soit d√©j√† dans le dashboard
update_task_status('TEST-001', 'üîÑ En cours')
"
```

**‚úÖ Validation:** Le dashboard doit montrer TEST-001 dans la section "üîÑ En cours"

---

#### √âtape 4: Pr√©parer la compl√©tion (cocher les sous-t√¢ches)

```bash
# Cocher toutes les sous-t√¢ches
sed -i '' 's/- \[ \]/- [x]/g' .tasks/test-e2e/TEST-001-sample-task.md

# Ajouter la section R√©sultat final (requise pour DoD)
echo "
---

## R√©sultat final

Test end-to-end compl√©t√© avec succ√®s. Toutes les √©tapes du workflow ont √©t√© valid√©es." >> .tasks/test-e2e/TEST-001-sample-task.md
```

**‚úÖ Validation:** V√©rifier que les sous-t√¢ches sont coch√©es et la section r√©sultat ajout√©e

---

#### √âtape 5: Valider avec DoD validator

```bash
uv run --with pyyaml python3 .claude/skills/task-management/scripts/validators/dod_validator.py TEST-001
```

**‚úÖ Validation:** Doit afficher "‚úÖ Valid: Yes" si toutes les conditions sont remplies

---

#### √âtape 6: Compl√©ter la t√¢che

```bash
# Mettre √† jour le statut dans le fichier
sed -i '' 's/üîÑ En cours/‚úÖ Termin√©/' .tasks/test-e2e/TEST-001-sample-task.md

# Mettre √† jour le dashboard
uv run --with pyyaml python3 -c "
import sys
sys.path.append('.claude/skills/task-management/scripts')
from core.dashboard_updater import update_task_status

update_task_status('TEST-001', '‚úÖ Termin√©')
"
```

**‚úÖ Validation:** Le dashboard doit montrer TEST-001 dans la section "‚úÖ T√¢ches termin√©es"

---

#### √âtape 7: Archiver la t√¢che

```bash
# D√©placer vers le dossier d'archives
mv .tasks/test-e2e/TEST-001-sample-task.md .tasks/test-e2e/.archived/

# V√©rifier l'archivage
ls -la .tasks/test-e2e/.archived/TEST-001-sample-task.md
```

**‚úÖ Validation:** Le fichier doit √™tre dans `.archived/` et ne plus √™tre dans le dossier principal

---

#### √âtape 8: Nettoyage

```bash
# Supprimer l'environnement de test
rm -rf .tasks/test-e2e

# D√©sactiver les variables d'environnement
unset TASK_SYSTEM_TASKS_DIR
unset TASK_SYSTEM_DASHBOARD
unset TASK_SYSTEM_ARCHIVED_DIR
```

**Status:** [ ]

*Note utilisateur:*

```markdown
[√Ä compl√©ter lors du test]
```

---

## Validation Finale Session 4

- [ ] Tous les tests Session 1-3 passent toujours
- [ ] 74 tests unitaires passent (100% success)
- [ ] Slash commands supprim√©s (9 fichiers, ~2,340 lignes)
- [ ] CLAUDE.md mis √† jour avec documentation skill
- [ ] Architecture progressive disclosure fonctionnelle
- [ ] Scripts Python isol√©s et testables
- [ ] Validation end-to-end compl√®te

**Valid√© par:** _______
**Date:** _______

---

## Notes de Migration

**Sessions compl√©t√©es:**

- ‚úÖ Session 1: Architecture + Infrastructure (config, file_parser, priority_scorer)
- ‚úÖ Session 2: Core + Validators (id_generator, dor/dod_validator, dashboard_updater, git_operations)
- ‚úÖ Session 3: Analysis + Lifecycle (recommendation_parser, 6 workflows)
- ‚è≥ Session 4: Tests + Documentation + Cleanup (en cours)

**M√©triques:**

- **Tests unitaires:** 74 (tous passent)
- **Workflows:** 9 (task-next, create, complete, from-analysis, analyze-source, start, archive, validate, from-idea)
- **Scripts Python:** 10 modules (~2,000 lignes)
- **R√©duction contexte:** ~2,340 lignes de slash commands ‚Üí ~1,900 lignes de workflows (r√©utilisables)
