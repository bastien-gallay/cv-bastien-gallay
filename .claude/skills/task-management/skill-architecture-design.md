# Task Management Skill - Architecture Design

**Version:** 2.0.0
**Date:** 2025-11-16
**Status:** Session 1 - Foundation
**Task:** INF-004

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [Progressive Disclosure (3 Levels)](#progressive-disclosure-3-levels)
4. [Directory Structure](#directory-structure)
5. [Task-Next Flow (Proof of Concept)](#task-next-flow-proof-of-concept)
6. [Reusable Patterns](#reusable-patterns)
7. [Technical Decisions](#technical-decisions)
8. [Migration Strategy](#migration-strategy)

---

## Executive Summary

This document describes the architecture for migrating the task management system from slash commands (`.claude/commands/`) to a unified Skill (`.claude/skills/task-management/`).

**Key Objectives:**

- **Optimize context**: Progressive disclosure loads only needed files
- **Improve maintainability**: Separate workflow (markdown) from logic (Python)
- **Enable testing**: Unit tests for algorithms, manual tests for workflows
- **Centralize configuration**: YAML data validated by Python dataclasses

**Session 1 Scope:**

- Establish architecture foundation
- Migrate `/task-next` as proof of concept
- Validate approach before full migration

---

## Architecture Overview

```mermaid
graph TB
    User[User] -->|invokes| Claude[Claude Code]
    Claude -->|loads| SkillMD[SKILL.md<br/>Level 1: Metadata]

    SkillMD -->|references| Workflows[workflows/*.md<br/>Level 2: Workflows]

    Workflows -->|calls| Scripts[scripts/**/*.py<br/>Level 3: Implementation]
    Workflows -->|reads| Config[config/*.yml<br/>Level 3: Configuration]
    Workflows -->|uses| Templates[templates/*.md<br/>Level 3: Templates]

    Scripts -->|loads| Config
    Scripts -->|runs| Tests[tests/*.py<br/>Unit Tests]

    User -->|validates| ManualTests[tests/MANUAL_TESTS.md<br/>User Validation]

    style SkillMD fill:#e1f5ff
    style Workflows fill:#fff4e6
    style Scripts fill:#f0f4c3
    style Config fill:#fce4ec
    style Tests fill:#e8f5e9
```

**Architecture Principles:**

1. **Separation of Concerns**: Workflow ‚â† Logic ‚â† Data
2. **Progressive Loading**: Only load what's needed
3. **Testability**: All logic is testable
4. **Configuration over Code**: Change behavior without touching Python

---

## Progressive Disclosure (3 Levels)

Claude Code's skill system uses progressive disclosure to minimize context usage:

```mermaid
graph LR
    L1[Level 1<br/>SKILL.md<br/>~200 tokens] -->|skill triggered| L2[Level 2<br/>workflow.md<br/>~1-2k tokens]
    L2 -->|as needed| L3A[Level 3<br/>scripts/<br/>~0 tokens*]
    L2 -->|as needed| L3B[Level 3<br/>config/<br/>~500 tokens]
    L2 -->|as needed| L3C[Level 3<br/>templates/<br/>~1k tokens]

    L3A -.->|output only| Context[Claude Context]
    L3B --> Context
    L3C --> Context

    style L1 fill:#ffebee
    style L2 fill:#fff3e0
    style L3A fill:#e8f5e9
    style L3B fill:#e8f5e9
    style L3C fill:#e8f5e9
```

**\*Note:** Scripts execute and return only **output**, not code. This is the key optimization.

### Level 1: Metadata (`SKILL.md`)

```yaml
---
name: task-management
description: Syst√®me de gestion de t√¢ches avec workflows Git, analyse comparative, et priorisation intelligente
version: 2.0.0
---

# Task Management Skill

[Brief overview with links to workflows/]
```

- **Always loaded** by Claude Code (~100-200 tokens)
- Contains metadata for skill discovery
- Links to Level 2 workflows

### Level 2: Workflows (`workflows/*.md`)

- **Loaded when command triggered** (~1-2k tokens each)
- Markdown files with user-facing instructions
- Call Python scripts as needed
- Handle user interaction (questionnaires, confirmations)
- Examples: `task-next.md`, `task-create.md`, etc.

### Level 3: Implementation (`scripts/`, `config/`, `templates/`)

- **Loaded on-demand** or **executed without loading code**
- Python scripts: executed, only **output** loaded (~0 tokens for code)
- Config YAML: loaded when needed (~500 tokens)
- Templates: loaded when needed (~1k tokens)

**Example flow for `/task-next`:**

```mermaid
User: /task-next
‚Üí Load SKILL.md (200 tokens)
‚Üí Load workflows/task-next.md (800 tokens)
‚Üí Execute scripts/algorithms/priority_scorer.py (0 tokens, output: 200 tokens)
‚Üí Load config/priorities.yml (300 tokens)

Total: ~1500 tokens vs 3000+ tokens for old slash command
```

---

## Directory Structure

```text
.claude/skills/task-management/
‚îú‚îÄ‚îÄ SKILL.md                              # Entry point (Level 1)
‚îú‚îÄ‚îÄ skill-architecture-design.md          # This document
‚îú‚îÄ‚îÄ README.md                             # User documentation
‚îÇ
‚îú‚îÄ‚îÄ config/                               # Configuration (Level 3)
‚îÇ   ‚îú‚îÄ‚îÄ priorities.yml                   # Priority definitions
‚îÇ   ‚îú‚îÄ‚îÄ trigrammes.yml                   # Trigramme categories
‚îÇ   ‚îî‚îÄ‚îÄ paths.yml                        # File paths
‚îÇ
‚îú‚îÄ‚îÄ workflows/                            # Workflows (Level 2)
‚îÇ   ‚îú‚îÄ‚îÄ task-next.md                     # [Session 1] Next task suggestion
‚îÇ   ‚îú‚îÄ‚îÄ task-create.md                   # [Session 2] Create task
‚îÇ   ‚îú‚îÄ‚îÄ task-from-idea.md                # [Session 2] From idea
‚îÇ   ‚îú‚îÄ‚îÄ task-start.md                    # [Session 2] Start task
‚îÇ   ‚îú‚îÄ‚îÄ task-complete.md                 # [Session 2] Complete task
‚îÇ   ‚îú‚îÄ‚îÄ task-validate.md                 # [Session 2] System validation
‚îÇ   ‚îú‚îÄ‚îÄ task-archive.md                  # [Session 3] Archive task
‚îÇ   ‚îú‚îÄ‚îÄ task-from-analysis.md            # [Session 3] From analysis
‚îÇ   ‚îî‚îÄ‚îÄ analyze-source.md                # [Session 3] Source extraction
‚îÇ
‚îú‚îÄ‚îÄ scripts/                              # Python implementation (Level 3)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                            # Core utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py            # [Session 1] Load & validate YAML ‚Üí dataclasses
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_parser.py              # [Session 1] Parse markdown task files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ id_generator.py             # [Session 2] Generate unique IDs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard_updater.py        # [Session 2] Update TASKS.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stats_calculator.py         # [Session 2] Calculate statistics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ git_operations.py           # [Session 2] Git helpers
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ validators/                      # Validation logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dor_validator.py            # [Session 2] Definition of Ready
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dod_validator.py            # [Session 2] Definition of Done
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ system_validator.py         # [Session 2] Full system validation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/                      # Algorithmic logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ priority_scorer.py          # [Session 1] WSJF scoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ branch_decider.py           # [Session 2] Branch creation logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commit_generator.py         # [Session 2] Smart commit messages
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analysis/                        # Analysis-specific logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommendation_parser.py    # [Session 3] Parse recommendations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ selection_parser.py         # [Session 3] Parse '1,5,6', 'all', etc.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_creator.py            # [Session 3] Batch task creation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analytics_updater.py        # [Session 3] Update ANALYSES.md
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                           # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ markdown_utils.py           # Markdown formatting helpers
‚îÇ       ‚îî‚îÄ‚îÄ date_utils.py               # Date parsing & calculations
‚îÇ
‚îú‚îÄ‚îÄ templates/                            # Templates (Level 3)
‚îÇ   ‚îú‚îÄ‚îÄ task.md                          # Task file template
‚îÇ   ‚îú‚îÄ‚îÄ commit-messages.md               # Commit templates
‚îÇ   ‚îî‚îÄ‚îÄ questionnaires/
‚îÇ       ‚îú‚îÄ‚îÄ task-create.md
‚îÇ       ‚îú‚îÄ‚îÄ task-from-idea.md
‚îÇ       ‚îî‚îÄ‚îÄ task-from-analysis.md
‚îÇ
‚îî‚îÄ‚îÄ tests/                                # Tests
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ MANUAL_TESTS.md                  # User validation scenarios
    ‚îÇ
    ‚îú‚îÄ‚îÄ test_config_loader.py            # [Session 1]
    ‚îú‚îÄ‚îÄ test_file_parser.py              # [Session 1]
    ‚îú‚îÄ‚îÄ test_priority_scorer.py          # [Session 1]
    ‚îÇ
    ‚îú‚îÄ‚îÄ test_id_generator.py             # [Session 2]
    ‚îú‚îÄ‚îÄ test_dashboard_updater.py        # [Session 2]
    ‚îú‚îÄ‚îÄ test_validators.py               # [Session 2]
    ‚îÇ
    ‚îú‚îÄ‚îÄ test_recommendation_parser.py    # [Session 3]
    ‚îÇ
    ‚îî‚îÄ‚îÄ fixtures/                        # Test data
        ‚îú‚îÄ‚îÄ sample_task.md
        ‚îú‚îÄ‚îÄ sample_tasks_dashboard.md
        ‚îî‚îÄ‚îÄ sample_recommendations.md
```

---

## Task-Next Flow (Proof of Concept)

This is the complete flow for `/task-next` command, demonstrating the architecture:

```mermaid
sequenceDiagram
    participant User
    participant Claude
    participant Workflow as workflows/task-next.md
    participant Scorer as scripts/algorithms/priority_scorer.py
    participant Parser as scripts/core/file_parser.py
    participant Config as config/priorities.yml
    participant TaskFiles as .tasks/tasks/*.md

    User->>Claude: /task-next
    Claude->>Workflow: Load (Level 2)

    Workflow->>Config: Load priorities config
    Config-->>Workflow: PriorityConfig data

    Workflow->>Scorer: Execute scoring algorithm

    Scorer->>Parser: parse_task_file() for each task
    Parser->>TaskFiles: Read task files
    TaskFiles-->>Parser: Raw markdown
    Parser-->>Scorer: Task metadata dict

    Scorer->>Scorer: Calculate scores<br/>(value/time formula)
    Scorer-->>Workflow: Ranked tasks (JSON)

    Workflow->>Workflow: Format output
    Workflow->>Claude: Display top 3 tasks
    Claude->>User: Show results

    alt User confirms --start
        User->>Claude: Confirm start
        Claude->>Workflow: Trigger /task-start
    end
```

### Data Flow Detail

**Input:**

- All task files in `.tasks/tasks/` with status "‚è≥ √Ä faire"
- Priority configuration from `config/priorities.yml`

**Processing:**

1. **Parse task files** (`file_parser.py`):

   ```python
   {
       "id": "QUA-001",
       "title": "√âtapes de v√©rification du CV",
       "status": "‚è≥ √Ä faire",
       "priority": "üü° Moyenne",
       "created_date": "2025-10-28",
       "target_date": None,
       "estimated_hours": 1.0
   }
   ```

2. **Calculate score** (`priority_scorer.py`):

   ```python
   value = (priority_score √ó 10) + (urgency √ó 5) + (age √ó 1)
   score = value / estimated_hours

   # Example:
   # priority=Moyenne (5) √ó 10 = 50
   # urgency=None (0) √ó 5 = 0
   # age=19 days (1.9) √ó 1 = 1.9
   # value = 51.9
   # time = 1 hour
   # score = 51.9 / 1 = 51.9
   ```

3. **Rank and filter**:

   ```python
   [
       TaskScore(id="QUA-001", score=51.9, ...),
       TaskScore(id="CNT-001", score=15.8, ...),
       TaskScore(id="INF-001", score=1.25, ...)
   ]
   ```

**Output:**

```markdown
üí° Prochaine t√¢che sugg√©r√©e: QUA-001

üìã √âtapes de v√©rification du CV
üü° Priorit√©: Moyenne
‚è±Ô∏è  Temps estim√©: 1 heure
üìÖ Cr√©√©e le: 2025-10-28

‚ú® Pourquoi cette t√¢che?
  - Ratio valeur/temps √©lev√© (Score: 51.9)
  - T√¢che courte (1h) et impactante

Autres t√¢ches √† consid√©rer:
  2. CNT-001 - Audit LinkedIn (Score: 15.8, üî¥ Haute, 4h)
  3. INF-001 - Int√©gration MCP (Score: 1.25, üü° Moyenne, 4h)
```

---

## Reusable Patterns

### Pattern 1: Config Loading (YAML + Python Dataclasses)

**Workflow:**

```mermaid
graph LR
    YAML[priorities.yml<br/>YAML Data] -->|parse| Loader[config_loader.py]
    Loader -->|validate| DC[PriorityConfig<br/>Dataclass]
    DC -->|cache| Memory[In-Memory Cache]
    Memory -.->|reuse| Scripts[Scripts]

    style YAML fill:#fce4ec
    style DC fill:#e1f5ff
    style Memory fill:#f0f4c3
```

**Implementation:**

```python
# config/priorities.yml
task_priorities:
  high:
    emoji: "üî¥"
    score: 10
    default_time_hours: 8
  medium:
    emoji: "üü°"
    score: 5
    default_time_hours: 4
```

```python
# scripts/core/config_loader.py
from dataclasses import dataclass
from pathlib import Path
import yaml

@dataclass
class PriorityLevel:
    emoji: str
    score: int
    default_time_hours: float

@dataclass
class PriorityConfig:
    high: PriorityLevel
    medium: PriorityLevel
    low: PriorityLevel

_cache = {}

def load_priorities() -> PriorityConfig:
    """Load and validate priorities from YAML."""
    if 'priorities' in _cache:
        return _cache['priorities']

    path = Path(__file__).parent.parent / "config" / "priorities.yml"
    with open(path) as f:
        data = yaml.safe_load(f)

    config = PriorityConfig(
        high=PriorityLevel(**data['task_priorities']['high']),
        medium=PriorityLevel(**data['task_priorities']['medium']),
        low=PriorityLevel(**data['task_priorities']['low'])
    )

    _cache['priorities'] = config
    return config
```

**Benefits:**

- ‚úÖ Data in YAML (easy to modify)
- ‚úÖ Validation at load time (Python types)
- ‚úÖ Cached (no re-parsing)
- ‚úÖ Type-safe usage in scripts

### Pattern 2: Markdown Parsing

**Common operations:**

- Extract metadata table (`| **Key** | Value |`)
- Extract subtasks (`- [ ]` / `- [x]`)
- Extract sections (`## Header`)
- Parse frontmatter

**Centralized in `file_parser.py`:**

```python
def parse_task_file(path: Path) -> dict:
    """Parse entire task file."""
    content = path.read_text()
    return {
        'metadata': parse_metadata_table(content),
        'subtasks': extract_subtasks(content),
        'description': extract_section(content, 'Description'),
        # ...
    }

def parse_metadata_table(content: str) -> dict:
    """Extract | **Key** | Value | table."""
    # Regex to match markdown table
    # Returns dict like {'Statut': '‚è≥ √Ä faire', ...}
    pass
```

**Usage across commands:**

- `/task-next`: Parse all task files for metadata
- `/task-start`: Parse for DoR validation
- `/task-complete`: Parse for DoD validation
- `/task-validate`: Parse all files for consistency

### Pattern 3: Dashboard Updates

**Common pattern:**

- Read `TASKS.md`
- Find entry by ID
- Update status/priority/date
- Recalculate statistics
- Write back atomically

**Centralized in `dashboard_updater.py`:**

```python
def update_task_status(task_id: str, new_status: str):
    """Update task status in TASKS.md."""
    # 1. Read file
    # 2. Find line with [task_id]
    # 3. Replace status emoji
    # 4. Recalculate stats
    # 5. Write atomically (temp file ‚Üí rename)
    pass

def recalculate_stats(content: str) -> dict:
    """Count tasks by status."""
    pass
```

### Pattern 4: Unit Testing

**Test structure:**

```python
# tests/test_priority_scorer.py
import pytest
from scripts.algorithms.priority_scorer import calculate_score, rank_tasks
from scripts.core.config_loader import load_priorities

def test_score_calculation_basic():
    """Test basic score calculation."""
    task = {
        'priority': 'medium',
        'estimated_hours': 2.0,
        'created_date': '2025-11-01',
        'target_date': None
    }
    config = load_priorities()

    score = calculate_score(task, config, today='2025-11-16')

    # priority=medium (5) √ó 10 = 50
    # urgency=none (0) √ó 5 = 0
    # age=15 days (1.5) √ó 1 = 1.5
    # value = 51.5
    # score = 51.5 / 2.0 = 25.75
    assert score.score == 25.75

def test_rank_tasks_empty_list():
    """Test ranking with no tasks."""
    assert rank_tasks([], load_priorities()) == []

def test_rank_tasks_equal_scores():
    """Test stable sorting when scores are equal."""
    # ... test tie-breaking logic
```

**Test fixtures:**

```text
tests/fixtures/
‚îú‚îÄ‚îÄ sample_task.md          # Minimal valid task
‚îú‚îÄ‚îÄ sample_tasks_dashboard.md  # TASKS.md sample
‚îî‚îÄ‚îÄ sample_recommendations.md  # Analysis recommendations
```

---

## Technical Decisions

### Decision 1: YAML + Python Dataclasses for Config

**Options considered:**

1. Pure YAML (data only, no validation)
2. Pure Python (dataclasses, no external data)
3. **YAML + Python validation** ‚úÖ **SELECTED**

**Rationale:**

- YAML: Easy to edit, no code changes needed
- Python dataclasses: Type safety, validation, IDE autocomplete
- Cache: Load once, reuse across commands

**Trade-offs:**

- ‚ùå Slightly more complex than pure Python
- ‚úÖ Much more flexible than hardcoded values
- ‚úÖ Best of both worlds

### Decision 2: Python for Algorithms, Markdown for Workflows

**Options considered:**

1. All Python (CLI-style)
2. All Markdown (instructions only)
3. **Python logic + Markdown workflows** ‚úÖ **SELECTED**

**Rationale:**

- Complex algorithms (WSJF, parsing) = Python (testable, fast)
- User interaction (questionnaires) = Markdown (Claude handles naturally)
- Clear separation of concerns

**Trade-offs:**

- ‚ùå Two languages to maintain
- ‚úÖ Each language used for its strength
- ‚úÖ Testable logic, readable workflows

### Decision 3: No Backup of Slash Commands

**Options considered:**

1. Auto-backup to `.claude/commands/.backup/`
2. Rename with `.legacy` suffix
3. **Git-only rollback** ‚úÖ **SELECTED**

**Rationale:**

- Git already provides full history
- No need for extra complexity
- Clean migration (delete old, add new)

**Trade-offs:**

- ‚ùå No "one-click" rollback
- ‚úÖ Simpler codebase
- ‚úÖ Git is single source of truth

### Decision 4: Simple Manual Tests (Not Detailed)

**Options considered:**

1. Detailed step-by-step procedures
2. **Simple command + expected result** ‚úÖ **SELECTED**
3. Interactive checklist with Claude

**Rationale:**

- Session 1 is proof of concept
- Simple tests faster to write and execute
- Can elaborate later if needed

**Trade-offs:**

- ‚ùå Less exhaustive coverage
- ‚úÖ Faster iteration
- ‚úÖ Good enough for validation

### Decision 5: Start with Design Doc (This Document)

**Options considered:**

1. **Design doc with Mermaid diagrams** ‚úÖ **SELECTED**
2. Direct implementation (code first)
3. Minimal prototype, then design

**Rationale:**

- Complex architecture needs planning
- Mermaid diagrams clarify flows
- User validation before coding
- Reference document for future sessions

**Trade-offs:**

- ‚ùå Time upfront (~15 min)
- ‚úÖ Shared understanding
- ‚úÖ Avoid rework
- ‚úÖ Living documentation

---

## Migration Strategy

### Session 1: Foundation (This Session)

**Goal:** Prove the architecture works

**Scope:**

- ‚úÖ Create skill structure
- ‚úÖ Establish config pattern (YAML + Python)
- ‚úÖ Migrate `/task-next` (simplest algorithmic command)
- ‚úÖ Create test patterns (unit + manual)

**Success Criteria:**

- `/task-next` via skill produces identical output to old command
- Unit tests pass
- Manual tests validated by user
- Architecture validated for Session 2

### Session 2: Core Infrastructure

**Goal:** Build reusable components

**Scope:**

- `scripts/core/` module (file_parser, id_generator, dashboard_updater, git_operations)
- `scripts/validators/` module (dor, dod, system)
- Migrate `/task-validate` (reuses validators)
- Expand test coverage

**Success Criteria:**

- Core utilities tested and reusable
- `/task-validate` functional
- Foundation for all other commands

### Session 3: Complex Commands

**Goal:** Migrate high-value commands

**Scope:**

- `/task-create`, `/task-from-idea` (interactive questionnaires)
- `/task-start`, `/task-complete` (lifecycle)
- `/task-from-analysis` (most complex, batch operations)
- Full test coverage

**Success Criteria:**

- All core commands migrated
- System fully functional
- Parallel operation with old commands validated

### Session 4: Cleanup & Documentation

**Goal:** Finalize migration

**Scope:**

- Remaining commands (`/analyze-source`, `/task-archive`)
- Update `CLAUDE.md` documentation
- Remove old slash commands
- End-to-end validation

**Success Criteria:**

- Old commands deleted
- Documentation complete
- User training complete
- System production-ready

---

## Next Steps (Session 1 Execution)

1. ‚úÖ **Review this design doc** (validate architecture)
2. ‚è≠Ô∏è Create skill structure (directories, `__init__.py`)
3. ‚è≠Ô∏è Implement config system (YAML + Python)
4. ‚è≠Ô∏è Implement `file_parser.py` with tests
5. ‚è≠Ô∏è Migrate WSJF algorithm to `priority_scorer.py` with tests
6. ‚è≠Ô∏è Create `workflows/task-next.md`
7. ‚è≠Ô∏è Create `SKILL.md` entry point
8. ‚è≠Ô∏è Create `MANUAL_TESTS.md`
9. ‚è≠Ô∏è Run all tests, validate with user
10. ‚è≠Ô∏è Commit Session 1

**Estimated time:** ~2-2.5 hours

---

## Appendix: Reference Links

- [Agent Skills Documentation](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview)
- [Progressive Disclosure](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)
- [Existing Priority Script](../../../scripts/priority_weight.py)
- [Task Rules (DoR/DoD)](../../.tasks/TASK_RULES.md)

---

**End of Design Document**
